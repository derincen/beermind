\documentclass[]{article}


%\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx} % more modern
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{titlesec}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage[noadjust]{cite}
\usepackage{natbib}
\bibliographystyle{unsrtnat}


\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\softmax}{softmax}



\begin{document}

% \title{Playing the Imitation Game:  A Recurrent Neural \\
% Network Chatbot with N-Sequence Memory}

\title{Sequence of (Sequence to Sequence) Learning: \\
Modeling Extended Dialogues with RNNs}

\author{Zachary C. Lipton$^1$, Sharad Vikram$^1$\\
University of California, San Diego\\
\texttt{\{zlipton, svikram\}@cs.ucsd.edu}
}

\date{June 5th, 2015}
\maketitle


\begin{abstract}
We present a word-level Recurrent Neural Network (RNN) architecture
for modeling dialogue across long conversations.
Unlike previous sequence to sequence models
our system captures dependencies across long exchanges.
Additionally, our setup allow each agent's part in an exchange
to be longer than a sentence.
This is accomplished by modeling punctuation marks as words.
In this paper, we fully explain the architecture,
empirically validate its utility,
and present web-based framework for perpetual learning
by interacting with humans in a web browser.


\end{abstract}


\section{Introduction}
\section{Rules of The Game}
In our model, dialogue consists
of \emph{exchanges} between two parties, called \emph{agents}.
Dialogue procedes as follows.
Each agent issues a block of text,
which we call a \emph{part}.
A \emph{part} may be as short as a single word or arbitrarily long.
In essence, a part is a sequence of \emph{tokens}.
\emph{Tokens} consist mostly of words,
but also include punctuation marks.
Each \emph{part} is terminated by a special token called \emph{<EOP>}.
Thus, the phrase ``Hey, want to grab coffee?"
would be encoded as follows: <hey> <,> <want> <to> <grab> <coffee> <?> <EOP>.

An \emph{exchange} is a pair of parts, issued one apiece by each of the agents.
An example exchange follows: \\
A: Hey, want to grab coffee?\\
B: Sure, where?\\
A conversation is modeled as simply a sequence of exchanges.

The model allows us to capture most aspects of two-party dialogue
as might occur in an online chat.
However, there are some minor restictions.
For example, our model has no notion of physical time.
Once agent A has issued a \emph{part},
it waits until B replies.
A has no opportunity to impatiently resume typing.
We are content to ignore this limitation here but note
that it can be cirvumvented
by setting conditions under which B's reply
is considered to be simply the end-of-part token <EOP>.


\section{The Sequence of (Sequence to Sequence) Model}
\section{Results}
\section{Perpetual Learning by Playing the Imitation Game}
\section{Conclusions}


\section{Problem with Encoder Decoder Models}
The LSTM model is very powerful because it is able to store ``memory" which is trappen in memory cells, cycling along ``constant error carousels". When we pass from encoder to decoder, we get only what is output by the encoder at the very last step, we lose all the information trapped in the carousels. The same goes when we pass from decoder to encoder. Maybe we need a more direct way of preserving model across the multiple exchanges?


\section{Dialogue Model}




\section{Introduction}
\section{Ideas}

\begin{itemize}


\item \textbf{ Sequence to Sequence to Sequence to ...} The fundamental idea of this work is two fold. One is that the sequence to sequence model is inherently flexible and need not apply to translation tasks. This insight was independently reached by the sequence-to-sequence authors themselves who applied the net to the question-to-answer setting. This bot is extremely impressive in it's ability to reply with relevant utterances to form a single exchange but needs to be reminded with each prompt what the context of the conversation is in order to have a passable interaction.

The truly novel idea here is that in contrast, our system connects multiple exchanges.
Consider the following conversation between two agents, A \& B:
L1  A:  “What’s your favorite vegetable?”
L2  B:  “I like cabbage, you?”
L3  A:  “I forget, what were we talking about?”
L4  B:  “You were asking what my favorite vegetable is.”

In our model of two party dialogue, a computer system could never learn to take the part of agent B. In order to answer the question from A in L3, it would need to remember what it had previously said in L1.

As LSTMs are specifically designed to learn long range time dependencies, we believe that we can train a system to have dialogues which span multiple exchanges in which the computer system retains information going all the way back to the very beginning of the conversation.

\item Previous work with RNNS to process text either worked at the character level (Sutskever, Karpathy) or word-level (using word2vec type embeddings) but was restricted to sentences. We desire to produce longer responses corresponding to one party's part in a two-party dialogue, but simultaneously to exploit the structure of word2vec representations.

Thus we model our text as a series of exchanges, where each exchange is a sequence of words (or tokens). In order to make our model maximally general we represent punctuation marks as tokens in the same class as words. Thus the sequence "hello, my name is Charlie Parker. I play bebop" whould be represented (<hello> <,> <my> <name> <is> <Charlie> <Parker> <.> <I> <play> <bebop> <EOR> where EOR is the only special token, one which signifies that the sentence is complete.

\item \textbf{Capitaization:} To capture captialization, we automatically capitalize words that come after periods in post-processing.

Note: This can get more complicated. Are we going to add words to the covab twice, once capitalized once not? What about words with distinct meanings, e.g., that are names but also verbs. One example is "Bob" and "bob".

\end{itemize}


\section{Conclusion}
\section{Acknowledgements}

\bibliography{deepx.bib}

\end{document}
